{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# EOPF: Sentinel‑2 UTM → HEALPix (Level‑1C)\n",
    "\n",
    "**Audience:** Researchers, Data Scientists\n",
    "**Last updated:** 2025-09-15\n",
    "\n",
    "This notebook demonstrates a clear, reproducible workflow to convert Sentinel‑2 L1C data\n",
    "from its native UTM projection to a global **HEALPix** grid, following the style of the\n",
    "[EOPF Sentinel‑2 examples].\n",
    "\n",
    "**You will learn to:**\n",
    "- Access cloud‑native Sentinel‑2 Zarr data (via STAC/EOPF)\n",
    "- Subset a region of interest (ROI)\n",
    "- Attach latitude/longitude coordinates to UTM gridded data\n",
    "- Reproject/aggregate onto a **HEALPix** equal‑area grid\n",
    "- Inspect and save the result for downstream analysis\n",
    "\n",
    "**Prerequisites:** Python, Xarray, familiarity with EO data, basic projections.\n",
    "\n",
    "> Tip: Run the notebook top‑to‑bottom in a fresh environment for best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Environment & Dependencies](#environment--dependencies)\n",
    "3. [Data Access via STAC](#data-access-via-stac)\n",
    "4. [Open the Sentinel‑2 Product](#open-the-sentinel2-product)\n",
    "5. [Subset a Region of Interest](#subset-a-region-of-interest)\n",
    "6. [Quicklook & Visualization](#quicklook--visualization)\n",
    "7. [Add Latitude/Longitude Coordinates](#add-latitudelongitude-coordinates)\n",
    "8. [Convert to HEALPix](#convert-to-healpix)\n",
    "9. [Inspect the HEALPix Output](#inspect-the-healpix-output)\n",
    "10. [Save & Export](#save--export)\n",
    "11. [Appendix / References](#appendix--references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "Sentinel‑2 Level‑1C products are distributed in **UTM** tiles (projected meters). Many global\n",
    "analyses benefit from a sphere‑aware, **equal‑area** grid. **HEALPix** (Hierarchical Equal Area\n",
    "isoLatitude Pixelization) partitions the globe into equal‑area cells, making aggregations and\n",
    "comparisons consistent across latitudes. This notebook restructures the original workflow with\n",
    "clear chapters and explanations while preserving all original code and outputs.\n",
    "\n",
    "This is a **Skelton** Notebook Converting EOPF Zarr format in UTM to HEALPix;\n",
    "We use EOPF Sample service data here.\n",
    "Since EOPF data is based on Datatree, we use that property. Thus this workflow can be applied to any UTM expressed EOPF ZARR format. (cf S2L1C and S2L2A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Environment & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xarray-eopf  xdggs healpix-geo flox numbagg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Imports and setup\n",
    "\n",
    "- Loads libraries: matplotlib, numpy, xarray. Configures plotting options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "import numpy as np\n",
    "import pyproj\n",
    "import pystac_client\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Data Access via STAC\n",
    "\n",
    "We query the EOPF STAC catalog for a suitable Sentinel‑2 L1C item (e.g., low cloud cover, good sun elevation) and obtain the Zarr asset. This keeps the workflow cloud‑native and lazily loaded.\n",
    "\n",
    "- To run this notebook with \"sentinel-2-l1c\", or \"sentinel-2-l2a\", simply specify it on the collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access cloud-optimized Sentinel-2 data via the EOPF STAC catalog\n",
    "catalog = pystac_client.Client.open(\"https://stac.core.eopf.eodc.eu\")\n",
    "\n",
    "# Define oceanographic study area and time window\n",
    "LON, LAT = -4.5, 48  # Bay of Biscay - known for consistent wave patterns\n",
    "date = \"2025-06-17/2025-06-17\"\n",
    "\n",
    "# Search criteria optimized for wave analysis\n",
    "\n",
    "collection = \"sentinel-2-l1c\"\n",
    "# collection = \"sentinel-2-l2a\"\n",
    "\n",
    "items = list(\n",
    "    catalog.search(\n",
    "        datetime=date,\n",
    "        collections=[collection],\n",
    "        intersects=dict(type=\"Point\", coordinates=[LON, LAT]),\n",
    "        query={\n",
    "            \"eo:cloud_cover\": {\n",
    "                \"lt\": 20\n",
    "            },  # Cloud cover < 20% ensures clear ocean surface\n",
    "            \"view:sun_elevation\": {\n",
    "                \"gt\": 25\n",
    "            },  # Filter for high sun elevation > 25° (→ sun zenith angle < 65°),\n",
    "            # which places the sun near the zenith.\n",
    "        },\n",
    "    ).items()\n",
    ")\n",
    "\n",
    "for item in items:\n",
    "    print(f\"✅ {item.id}\")\n",
    "\n",
    "item = items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Open the Sentinel‑2 Product\n",
    "\n",
    "We open the product directly as an **Xarray DataTree** (lazy) to navigate measurements, conditions, and multi‑resolution groups (10m/20m/60m).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset lazily from object storage\n",
    "dt = xr.open_datatree(\n",
    "    item.assets[\"product\"].href,\n",
    "    **item.assets[\"product\"].extra_fields[\"xarray:open_datatree_kwargs\"],\n",
    ")\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Subset a Region of Interest\n",
    "\n",
    "We extract a small ROI (e.g., a size×size window) to demonstrate transformation and HEALPix conversion on a compact example.\n",
    "\n",
    "If you use here 'size'=3, the notebook runs on pangeo-eosc cluster less than 30 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose small area in UTM\n",
    "## TODO, we can update here to chose  'one detection' area.\n",
    "size = 23\n",
    "size = 12\n",
    "size = 3\n",
    "\n",
    "x_min = 0\n",
    "x_max = x_min + (size - 1)\n",
    "y_max = 0\n",
    "y_min = y_max + (size - 1)\n",
    "print(x_min, x_max, y_min, y_max)\n",
    "\n",
    "utm_x_min = dt[\"conditions\"][\"geometry\"][\"sun_angles\"].x[x_min]\n",
    "utm_x_max = dt[\"conditions\"][\"geometry\"][\"sun_angles\"].x[x_max]\n",
    "utm_y_min = dt[\"conditions\"][\"geometry\"][\"sun_angles\"].y[y_min]\n",
    "utm_y_max = dt[\"conditions\"][\"geometry\"][\"sun_angles\"].y[y_max]\n",
    "print(utm_x_min, utm_x_max, utm_y_min, utm_y_max)\n",
    "\n",
    "\n",
    "small_dt = dt.sel(\n",
    "    x=slice(\n",
    "        utm_x_min,\n",
    "        utm_x_max,\n",
    "    ),\n",
    "    y=slice(\n",
    "        utm_y_max,\n",
    "        utm_y_min,\n",
    "    ),\n",
    ")\n",
    "small_dt  # 5299995, 5290205"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Quicklook & Visualization\n",
    "\n",
    "We visualize one or more bands (e.g., B02) to confirm the ROI and interpret pixel values before reprojection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (small_dt['quality']['l1c_quicklook']['r10m']['tci'].hvplot.rgb(x='x',y='y',bands='band',  )+small_dt['measurements']['reflectance']['r10m']['b02'].hvplot(x='x',y='y', ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Add Latitude/Longitude Coordinates\n",
    "\n",
    "Using **pyproj** we transform UTM (x/y in meters) to geographic coordinates (lon/lat in degrees) and attach them as auxiliary coordinates. This enables subsequent binning onto a spherical grid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Annotate UTM with latitude and Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_latlon(ds: xr.Dataset, transformer: pyproj.Transformer) -> xr.Dataset:\n",
    "    \"\"\"Attach latitude/longitude coords + CF metadata to a Dataset with (x,y).\"\"\"\n",
    "    if not {\"x\", \"y\"}.issubset(ds.dims):\n",
    "        return ds\n",
    "\n",
    "    xx, yy = np.meshgrid(ds[\"x\"].values, ds[\"y\"].values, indexing=\"xy\")\n",
    "    lon, lat = transformer.transform(xx, yy)\n",
    "\n",
    "    ds = ds.assign_coords(\n",
    "        longitude=((\"y\", \"x\"), lon),\n",
    "        latitude=((\"y\", \"x\"), lat),\n",
    "    )\n",
    "    ds[\"latitude\"].attrs.update(\n",
    "        {\n",
    "            \"standard_name\": \"latitude\",\n",
    "            \"long_name\": \"Latitude\",\n",
    "            \"units\": \"degrees_north\",\n",
    "            \"axis\": \"Y\",\n",
    "        }\n",
    "    )\n",
    "    ds[\"longitude\"].attrs.update(\n",
    "        {\n",
    "            \"standard_name\": \"longitude\",\n",
    "            \"long_name\": \"Longitude\",\n",
    "            \"units\": \"degrees_east\",\n",
    "            \"axis\": \"X\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Make sure vars with (y,x) advertise the aux coords\n",
    "    for var in ds.data_vars:\n",
    "        if {\"y\", \"x\"}.issubset(ds[var].dims):\n",
    "            existing = ds[var].attrs.get(\"coordinates\", \"\").split()\n",
    "            ds[var].attrs[\"coordinates\"] = \" \".join(\n",
    "                sorted(set(existing) | {\"latitude\", \"longitude\"})\n",
    "            )\n",
    "    return ds\n",
    "\n",
    "\n",
    "def add_latlon(\n",
    "    path: str, ds: xr.Dataset, transformer: pyproj.Transformer\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"Wrapper for safe application on a node dataset.\"\"\"\n",
    "    if ds is None:\n",
    "        print(path, \"no dataset\")\n",
    "        return ds\n",
    "    if not {\"x\", \"y\"}.issubset(ds.dims):\n",
    "        print(path, \"not both x,y\")\n",
    "        return ds\n",
    "    return _add_latlon(ds, transformer)\n",
    "\n",
    "\n",
    "def add_latlon_to_dt(dt: xr.DataTree) -> xr.DataTree:\n",
    "    \"\"\"Return a new DataTree with latitude/longitude coords added everywhere possible.\"\"\"\n",
    "    crs_code = dt.attrs[\"other_metadata\"][\"horizontal_CRS_code\"]\n",
    "    src_crs = pyproj.CRS.from_string(crs_code)\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        src_crs, pyproj.CRS.from_epsg(4326), always_xy=True\n",
    "    )\n",
    "    return xr.DataTree.from_dict(\n",
    "        {\n",
    "            path: add_latlon(path, node.ds, transformer)\n",
    "            for path, node in dt.subtree_with_keys\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "latlon_dt = add_latlon_to_dt(small_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_dt[\"measurements\"][\"reflectance\"][\"r10m\"][\"b02\"].plot(\n",
    "    x=\"longitude\", y=\"latitude\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon_dt[\"measurements\"][\"reflectance\"][\"r10m\"][\"b02\"].plot(\n",
    "    x=\"longitude\", y=\"latitude\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Convert to HEALPix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to HEALPix.\n",
    "# Todo here: add 'data_tree' branch that indicate 'Healpix_level', i.e. instead of r10m,\n",
    "\n",
    "\n",
    "from healpix_geo.nested import lonlat_to_healpix\n",
    "\n",
    "# --- level selection (coarsest grid not finer than dx) ---\n",
    "EARTH_RADIUS_M = 6_371_000.0  # radius used in healpix-geo levels table\n",
    "\n",
    "\n",
    "def _healpix_edge_length_m(level: int, radius_m: float = EARTH_RADIUS_M) -> float:\n",
    "    # edge = R * sqrt(pi/3) / 2**level  (matches healpix-geo \"levels\" page)\n",
    "    return radius_m * np.sqrt(np.pi / 3.0) / (2**level)\n",
    "\n",
    "\n",
    "def _infer_dx_from_x(ds: xr.Dataset) -> float:\n",
    "    x = np.asarray(ds[\"x\"].values)\n",
    "    dx = float(np.nanmedian(np.abs(np.diff(x))))\n",
    "    if not np.isfinite(dx) or dx <= 0:\n",
    "        raise ValueError(\"Could not infer a positive spacing from ds['x'].\")\n",
    "    return dx\n",
    "\n",
    "\n",
    "def choose_healpix_level_from_dx(\n",
    "    ds: xr.Dataset, min_level: int = 0, max_level: int = 29\n",
    ") -> int:\n",
    "    dx = _infer_dx_from_x(ds)\n",
    "    base = EARTH_RADIUS_M * np.sqrt(np.pi / 3.0)\n",
    "    level = int(np.floor(np.log2(base / dx)))  # edge(level) >= dx\n",
    "    return int(np.clip(level, min_level, max_level))\n",
    "\n",
    "\n",
    "# --- single-dataset transform -> grouped by unique HEALPix cell_ids ---\n",
    "def to_healpix_cells_grouped_mean(\n",
    "    ds: xr.Dataset, level: int | None = None, ellipsoid: str = \"WGS84\"\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Returns a dataset with dims (angle, cell_ids), where 'cell_ids' is a\n",
    "    dimension/coordinate containing unique HEALPix ids (NESTED).\n",
    "    Values are averaged over all source samples that mapped to the same cell.\n",
    "    \"\"\"\n",
    "    if not {\"y\", \"x\"}.issubset(ds.dims):\n",
    "        raise ValueError(\"Dataset must have 'y' and 'x' dimensions.\")\n",
    "    if not {\"latitude\", \"longitude\"}.issubset(ds.coords):\n",
    "        raise ValueError(\n",
    "            \"Dataset must have 'latitude' and 'longitude' coords (degrees).\"\n",
    "        )\n",
    "\n",
    "    if level is None:\n",
    "        level = choose_healpix_level_from_dx(ds)\n",
    "\n",
    "    # 1) hash each (lon,lat) to HEALPix nested cell id\n",
    "    lon = ds[\"longitude\"].values.ravel()\n",
    "    lat = ds[\"latitude\"].values.ravel()\n",
    "    cell_ids = lonlat_to_healpix(lon, lat, level, ellipsoid=ellipsoid)\n",
    "\n",
    "    # 2) stack (y,x) -> cells\n",
    "    out = ds.stack(cells=(\"y\", \"x\"))\n",
    "\n",
    "    # 3) attach cell_ids coord on 'cells'\n",
    "    out = out.assign_coords(cell_ids=(\"cells\", cell_ids.astype(\"int64\")))\n",
    "    out[\"cell_ids\"].attrs.update(\n",
    "        {\n",
    "            \"grid_name\": \"healpix\",\n",
    "            \"level\": level,\n",
    "            \"indexing_scheme\": \"nested\",\n",
    "        }\n",
    "    )\n",
    "    cell_ids_attrs = dict(out[\"cell_ids\"].attrs)  # keep for after groupby\n",
    "\n",
    "    # 4) drop redundant coords/vars\n",
    "    #    drop_these = [n for n in (\"x\", \"y\", \"latitude\", \"longitude\") if n in out.variables]\n",
    "    #    out = out.drop_vars(drop_these)\n",
    "\n",
    "    # 5) group by cell_ids and average -> new dim named 'cell_ids'\n",
    "    # **note** This is a very simplified test conversion, later this should be updated spline other interpolated method.\n",
    "    out = out.groupby(\"cell_ids\").mean()\n",
    "\n",
    "    # 6) restore attrs on the new dimension coordinate\n",
    "    if \"cell_ids\" in out.coords:\n",
    "        out[\"cell_ids\"].attrs.update(cell_ids_attrs)\n",
    "\n",
    "    # 7) keep order stable for variables like (angle, cell_ids)\n",
    "    #   for v in out.data_vars:\n",
    "    #       if (\"angle\" in out[v].dims) and (\"cell_ids\" in out[v].dims):\n",
    "    #           out[v] = out[v].transpose(\"angle\", \"cell_ids\", ...)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# --- per-node handler for the DataTree pass ---\n",
    "def _add_healpix_to_dt_node(path: str, ds: xr.Dataset) -> xr.Dataset:\n",
    "    if ds is None:\n",
    "        print(path, \"no dataset — keeping empty node\")\n",
    "        return xr.Dataset()\n",
    "\n",
    "    has_xy = {\"x\", \"y\"}.issubset(ds.dims)\n",
    "    has_ll = {\"latitude\", \"longitude\"}.issubset(ds.coords)\n",
    "\n",
    "    if has_xy and not has_ll:\n",
    "        # stop the whole operation as requested\n",
    "        raise RuntimeError(\n",
    "            f\"{path}: has x/y but missing latitude/longitude — aborting.\"\n",
    "        )\n",
    "\n",
    "    if has_ll and has_xy:\n",
    "        depth = choose_healpix_level_from_dx(ds)\n",
    "        print(\n",
    "            f\"{path}: chosen level {depth} (edge≈{_healpix_edge_length_m(depth):.4f} m)\"\n",
    "        )\n",
    "        return to_healpix_cells_grouped_mean(ds, level=depth, ellipsoid=\"WGS84\")\n",
    "\n",
    "    # no lat/lon -> do nothing\n",
    "    print(path, \"no latitude/longitude — skipping\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "# --- public function: apply over the whole DataTree ---\n",
    "def add_healpix_to_dt(dt: xr.DataTree) -> xr.DataTree:\n",
    "    \"\"\"Transform nodes to HEALPix (grouped mean per cell) where possible; preserve others.\"\"\"\n",
    "    mapping = {\n",
    "        path: _add_healpix_to_dt_node(path, node.ds)\n",
    "        for path, node in dt.subtree_with_keys\n",
    "        # Here re-name pass if it is\n",
    "    }\n",
    "    return xr.DataTree.from_dict(mapping, name=getattr(dt, \"name\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert the whole tree\n",
    "healpix_dt = add_healpix_to_dt(latlon_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Inspect the HEALPix Output\n",
    "\n",
    "We check dimensions and metadata (e.g., `cell_ids`, level, indexing scheme) and ensure variables were preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a specific node you know had lon/lat + x/y\n",
    "healpix_dt[\"measurements\"][\"reflectance\"][\"r10m\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Save & Export\n",
    "\n",
    "We persist the HEALPix‑indexed data as **Zarr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# todo rechunk here (may be re-use rechunk function justus will propose for cliamate dt for optimal chunking of healpix?)\n",
    "#\n",
    "healpix_dt.to_zarr(collection + \"HEALPix.zarr\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_datatree(collection + \"HEALPix.zarr\", engine=\"zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Note: goto next notbook made by benoit on updating metadata?\n",
    "\n",
    "- The bouding box of this area, if it exist in stac metadata, should follow the new shape defined by healpix (zig zag polygon due to healpix pixels\n",
    "- r10m or such now in healpix, thus it should be updated with 19 and metadata on parents indicating '19' is a reference_level\n",
    "\n",
    "\n",
    "### Store the DT in right chunk for HEALPix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Appendix / References\n",
    "\n",
    "- EOPF Sample Notebooks (Sentinel‑2): examples of structure and style for cloud‑native EO workflows.\n",
    "- HEALPix: Górski et al., 2005. *ApJ* 622, 759–771.\n",
    "- PyProj/PROJ: Coordinate transforms between projected and geographic systems.\n",
    "- Xarray DataTree: Hierarchical datasets for multi‑group EO products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
