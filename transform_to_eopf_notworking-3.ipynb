{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "The docker image is availabe with following path\n",
    "```\n",
    "docker.io/tinaok/eopf-webiner6-docker:eopf\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvf /home/jovyan/grid4earth_S2L1B/Sentinel-2/MSI/MSI_L1B_GR/2025/07/24/S2A_OPER_MSI_L1B_GR_2APS_20250724T145646_S20250724T112807_D06_N05.11.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv  S2A_OPER_MSI_L1B_GR_2APS_20250724T145646_S20250724T112807_D06_N05.11 S2A_OPER_MSI_L1B_GR_2APS_20250724T145646_S20250724T112807_D06_N05.11.SAFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from eopf.store.convert import convert\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "safe_filename = \"./Sentinel-2/MSI/MSI_L1B_GR/2025/07/24/S2A_OPER_MSI_L1B_GR_2APS_20250724T011624_S20250724T000002_D01_N05.11.tar\"\n",
    "# safe_filename='./Sentinel-2/MSI/MSI_L1B_GR/2025/07/24/S2A_OPER_MSI_L1B_GR_2APS_20250724T145646_S20250724T112807_D06_N05.11.SAFE'\n",
    "safe_filename = (\n",
    "    \"S2A_OPER_MSI_L1B_GR_2APS_20250724T145646_S20250724T112807_D06_N05.11.SAFE\"\n",
    ")\n",
    "zarr_filename = os.path.split(safe_filename)[1].replace(\".SAFE\", \".zarr\")\n",
    "zarr_path = os.path.join(current_dir, zarr_filename)\n",
    "print(zarr_path)\n",
    "# convert( safe_filename, zarr_path, mask_and_scale=True)\n",
    "# safe_filename = 'tar:///home/jovyan/grid4earth_S2L1B/Sentinel-2/MSI/MSI_L1B_GR/2025/07/24/S2A_OPER_MSI_L1B_GR_2APS_20250724T145646_S20250724T112811_D06_N05.11.tar!S2A_OPER_MSI_L1B_GR_2APS_20250724T145646_S20250724T112811_D06_N05.11.SAFE'\n",
    "\n",
    "print(safe_filename)\n",
    "convert(\n",
    "    safe_filename,\n",
    "    zarr_path,\n",
    "    mask_and_scale=True,\n",
    "    source_store_kwargs={\n",
    "        \"product_level\": \"L1B\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "import pystac_client\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Setup and Environment Configuration\n",
    "\n",
    "First, we'll import the required libraries and set up our environment. Make sure your Copernicus credentials are stored in your environment variables or a `.env` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get credentials from environment variables\n",
    "load_dotenv()\n",
    "ACCESS_KEY_ID = os.environ.get(\"ACCESS_KEY_ID\")\n",
    "SECRET_ACCESS_KEY = os.environ.get(\"SECRET_ACCESS_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### The S3Connector Class\n",
    "\n",
    "The `S3Connector` class provides an interface to connect to the S3-compatible storage service of the Copernicus Data Space Ecosystem. This class handles authentication and connection management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3Connector:\n",
    "    \"\"\"A clean connector for S3-compatible storage services\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, endpoint_url, access_key_id, secret_access_key, region_name=\"default\"\n",
    "    ):\n",
    "        \"\"\"Initialize the S3Connector with connection parameters\"\"\"\n",
    "        self.endpoint_url = endpoint_url\n",
    "        self.access_key_id = access_key_id\n",
    "        self.secret_access_key = secret_access_key\n",
    "        self.region_name = region_name\n",
    "\n",
    "        # Create session\n",
    "        self.session = boto3.session.Session()\n",
    "\n",
    "        # Initialize S3 resource\n",
    "        self.s3 = self.session.resource(\n",
    "            \"s3\",\n",
    "            endpoint_url=self.endpoint_url,\n",
    "            aws_access_key_id=self.access_key_id,\n",
    "            aws_secret_access_key=self.secret_access_key,\n",
    "            region_name=self.region_name,\n",
    "        )\n",
    "\n",
    "        # Initialize S3 client\n",
    "        self.s3_client = self.session.client(\n",
    "            \"s3\",\n",
    "            endpoint_url=self.endpoint_url,\n",
    "            aws_access_key_id=self.access_key_id,\n",
    "            aws_secret_access_key=self.secret_access_key,\n",
    "            region_name=self.region_name,\n",
    "        )\n",
    "\n",
    "    def get_s3_client(self):\n",
    "        \"\"\"Get the boto3 S3 client\"\"\"\n",
    "        return self.s3_client\n",
    "\n",
    "    def get_s3_resource(self):\n",
    "        \"\"\"Get the boto3 S3 resource\"\"\"\n",
    "        return self.s3\n",
    "\n",
    "    def get_bucket(self, bucket_name):\n",
    "        \"\"\"Get a specific bucket by name\"\"\"\n",
    "        return self.s3.Bucket(bucket_name)\n",
    "\n",
    "    def list_buckets(self):\n",
    "        \"\"\"List all available buckets\"\"\"\n",
    "        response = self.s3_client.list_buckets()\n",
    "        if \"Buckets\" in response:\n",
    "            return [bucket[\"Name\"] for bucket in response[\"Buckets\"]]\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "The following function helps convert S3 URIs from the STAC catalog into S3 keys that can be used for direct access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_s3_path_from_url(url):\n",
    "    \"\"\"\n",
    "    Extracts the S3 object path from an S3 URL or URI.\n",
    "\n",
    "    This function parses S3 URLs/URIs and returns just the object path portion,\n",
    "    removing the protocol (s3://), bucket name, and any leading slashes.\n",
    "\n",
    "    Args:\n",
    "        url (str): The full S3 URI (e.g., 's3://eodata/path/to/file.jp2')\n",
    "\n",
    "    Returns:\n",
    "        str: The S3 object path (without protocol, bucket name and leading slashes)\n",
    "    \"\"\"\n",
    "    # If it's not an S3 URI, return it unchanged\n",
    "    if not url.startswith(\"s3://\"):\n",
    "        return url\n",
    "\n",
    "    # Parse the S3 URI\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    # Ensure this is an S3 URL\n",
    "    if parsed_url.scheme != \"s3\":\n",
    "        raise ValueError(f\"URL {url} is not an S3 URL\")\n",
    "\n",
    "    # Extract the path without leading slashes\n",
    "    object_path = parsed_url.path.lstrip(\"/\")\n",
    "\n",
    "    return object_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product(s3_resource, bucket_name, object_url, output_path):\n",
    "    \"\"\"\n",
    "    Download a product from S3 bucket and create output directory if it doesn't exist.\n",
    "\n",
    "    Args:\n",
    "        s3_resource: boto3 S3 resource object\n",
    "        bucket_name (str): Name of the S3 bucket\n",
    "        object_url (str): Path to the object within the bucket\n",
    "        output_path (str): Local directory to save the file\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the downloaded file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    # Extract filename from the object URL\n",
    "    _, filename = os.path.split(object_url)\n",
    "\n",
    "    # Full path where the file will be saved\n",
    "    local_file_path = os.path.join(output_path, filename)\n",
    "\n",
    "    print(f\"Downloading {object_url} to {local_file_path}...\")\n",
    "\n",
    "    try:\n",
    "        # Download the file from S3\n",
    "        s3_resource.Bucket(bucket_name).download_file(object_url, local_file_path)\n",
    "        print(f\"Successfully downloaded to {local_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    return local_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Connecting to Copernicus Data Space Ecosystem\n",
    "\n",
    "Now let's establish connections to both the S3 storage and STAC catalog services using our credentials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY_ID = os.environ.get(\"ACCESS_KEY_ID\")\n",
    "SECRET_ACCESS_KEY = os.environ.get(\"SECRET_ACCESS_KEY\")\n",
    "ENDPOINT_URL = \"https://eodata.dataspace.copernicus.eu\"\n",
    "ENDPOINT_STAC = \"https://stac.dataspace.copernicus.eu/v1/\"\n",
    "BUCKET_NAME = \"eodata\"\n",
    "catalog = pystac_client.Client.open(ENDPOINT_STAC)\n",
    "connector = S3Connector(\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    access_key_id=ACCESS_KEY_ID,\n",
    "    secret_access_key=SECRET_ACCESS_KEY,\n",
    "    region_name=\"default\",\n",
    ")\n",
    "# Get S3 client and resource from the connector instance\n",
    "s3 = connector.get_s3_resource()\n",
    "s3_client = connector.get_s3_client()\n",
    "buckets = connector.list_buckets()\n",
    "print(\"Available buckets:\", buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Searching for Sentinel-2 Product\n",
    "\n",
    "We'll use the STAC API to search for Sentinel-2 Level 2A products based on:\n",
    "- Geographic location (longitude/latitude point)\n",
    "- Date range\n",
    "- Cloud cover threshold\n",
    "\n",
    "The search results provide metadata and access links to the actual imagery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "\n",
    "LON, LAT = -4.5, 48\n",
    "\n",
    "date = \"2025-07-07/2025-07-11\"\n",
    "date = \"2025-06-17/2025-06-17\"\n",
    "date = \"2025-06-17/2025-06-17\"\n",
    "date = \"2025-07-24/2025-07-24\"\n",
    "\n",
    "\n",
    "collections = [\"sentinel-2-l1c\"]\n",
    "query = {\n",
    "    \"eo:cloud_cover\": {\"lt\": 20},\n",
    "}\n",
    "# Search for Sentinel-2 products\n",
    "items = catalog.search(\n",
    "    collections=collections, intersects=Point(LON, LAT), datetime=date, query=query\n",
    ").item_collection()\n",
    "for item in items:\n",
    "    print(f\"âœ… {item.id}\")\n",
    "selected_item = items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Downloading Sentinel-2 Product\n",
    "\n",
    "Once we've identified the product we want, we can download it using our S3 connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(bucket, product: str, target: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Downloads every file in the S3 bucket with the provided product prefix.\n",
    "    Creates a local folder named after the .SAFE directory (without the .SAFE extension).\n",
    "\n",
    "    Args:\n",
    "        bucket: boto3 Resource bucket object\n",
    "        product: Path to the product (e.g., 'Sentinel-2/MSI/L2A/.../S2B_MSIL2A_..._T56KKB_20240516T015827.SAFE/')\n",
    "        target: Local directory to save the files. Defaults to current directory.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to the downloaded .SAFE directory (without the .SAFE extension)\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the product was not found in the bucket\n",
    "    \"\"\"\n",
    "    # Ensure the product path ends with '/'\n",
    "    if not product.endswith(\"/\"):\n",
    "        product += \"/\"\n",
    "\n",
    "    # List files in the S3 prefix\n",
    "    files = list(bucket.objects.filter(Prefix=product))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Could not find any files for {product}\")\n",
    "\n",
    "    # Extract the .SAFE directory name (e.g., \"S2B_MSIL2A_20240516T001109_N0510_R073_T56KKB_20240516T015827.SAFE\")\n",
    "    safe_dir = os.path.basename(product.rstrip(\"/\"))\n",
    "    if not safe_dir.endswith(\".SAFE\"):\n",
    "        raise ValueError(f\"Expected a .SAFE directory, got: {safe_dir}\")\n",
    "\n",
    "    # Create the local target directory (without the .SAFE extension)\n",
    "    # local_dir = safe_dir[:-5]  # Remove '.SAFE' from the name\n",
    "    local_path = os.path.join(target, safe_dir)\n",
    "\n",
    "    # Create the local directory structure\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "    # Download each file while preserving the relative structure\n",
    "    for file in files:\n",
    "        # Skip directory markers (S3 pseudo-folders)\n",
    "        if file.key.endswith(\"/\"):\n",
    "            continue\n",
    "\n",
    "        # Compute the relative path inside the .SAFE directory\n",
    "        relative_path = os.path.relpath(file.key, product)\n",
    "        local_file_path = os.path.join(local_path, relative_path)\n",
    "\n",
    "        # Create parent directories if they don't exist\n",
    "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "        # Download the file\n",
    "        bucket.download_file(file.key, local_file_path)\n",
    "\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = s3.Bucket(BUCKET_NAME)\n",
    "product_url, _ = os.path.split(selected_item.assets[\"safe_manifest\"].href)\n",
    "product_url = extract_s3_path_from_url(product_url)\n",
    "safe_filename = download(bucket, product_url, target=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Replace .SAFE with .zarr\n",
    "zarr_filename = os.path.split(safe_filename)[1].replace(\".SAFE\", \".zarr\")\n",
    "# Join the current directory path with the new filename\n",
    "zarr_path = os.path.join(current_dir, zarr_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eopf.store.convert import convert\n",
    "\n",
    "convert(safe_filename, zarr_path, mask_and_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
